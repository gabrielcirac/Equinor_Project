{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e3207e-b709-4b59-93ca-323053f2f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Carregadas 6 arquiteturas e 5 perfis de hiperparâmetros.\n"
     ]
    }
   ],
   "source": [
    "# %%capture capturado\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "# %%capture --no-stderr\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "\"\"\"Suppress TensorFlow and addon warnings for a cleaner console.\"\"\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['ABSL_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=UserWarning,\n",
    "    module='tensorflow_addons'\n",
    ")\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from colorlog import ColoredFormatter\n",
    "\n",
    "from forecast_pipeline.config import LOG_LEVEL, DEFAULT_EXP_PARAMS\n",
    "from forecast_pipeline.jobs import (\n",
    "    generate_jobs,\n",
    "    select_data_sources,\n",
    "    create_filter_configurations,\n",
    ")\n",
    "from forecast_pipeline.metrics import (\n",
    "    collate_metrics,\n",
    "    clean_and_structure_results,\n",
    ")\n",
    "from forecast_pipeline.io_utils import (\n",
    "    generate_experiment_name,\n",
    "    save_experiment_to_excel,\n",
    ")\n",
    "from forecast_pipeline.runner import execute_jobs, run_experiments_for_config\n",
    "from common.config_wells import DATA_SOURCES\n",
    "from forecast_pipeline.config import DEFAULT_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7b037-392c-46f3-ab06-38110e485f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Starting main pipeline…\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] No filter methods provided; running with no adaptive filtering.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Dispatching 50 jobs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3c625a9dc74820b88c83ab177acb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Jobs completed:   0%|          | 0/50 [00:00<?, ?job/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Attempting to load UNISIM-IV data for well 'P16' from /home/gabriel/Documentos/Equinor/data/UNISIM-IV-2026/Well_P16_UNISIM-IV.csv\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Attempting to load UNISIM-IV data for well 'P16' from /home/gabriel/Documentos/Equinor/data/UNISIM-IV-2026/Well_P16_UNISIM-IV.csv\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Attempting to load UNISIM-IV data for well 'P16' from /home/gabriel/Documentos/Equinor/data/UNISIM-IV-2026/Well_P16_UNISIM-IV.csv\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Attempting to load UNISIM-IV data for well 'P16' from /home/gabriel/Documentos/Equinor/data/UNISIM-IV-2026/Well_P16_UNISIM-IV.csv\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Attempting to load UNISIM-IV data for well 'P16' from /home/gabriel/Documentos/Equinor/data/UNISIM-IV-2026/Well_P16_UNISIM-IV.csv\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Attempting to load UNISIM-IV data for well 'P16' from /home/gabriel/Documentos/Equinor/data/UNISIM-IV-2026/Well_P16_UNISIM-IV.csv\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Successfully read CSV: 1796 rows, 5 columns\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Successfully read CSV: 1796 rows, 5 columns\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Successfully read CSV: 1796 rows, 5 columns\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Successfully read CSV: 1796 rows, 5 columns\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Successfully read CSV: 1796 rows, 5 columns\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Successfully read CSV: 1796 rows, 5 columns\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Renamed 'Day' to 'Tempo_Inicio_Prod'\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Renamed 'Day' to 'Tempo_Inicio_Prod'\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Renamed 'Day' to 'Tempo_Inicio_Prod'\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Renamed 'Day' to 'Tempo_Inicio_Prod'\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Renamed 'Day' to 'Tempo_Inicio_Prod'\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:41 [INFO   ] Renamed 'Day' to 'Tempo_Inicio_Prod'\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Added UNISIM-IV features (delta_P, PI, etc.)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Added UNISIM-IV features (delta_P, PI, etc.)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Added UNISIM-IV features (delta_P, PI, etc.)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Normalized DataFrame to canonical feature set\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Normalized DataFrame to canonical feature set\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Added UNISIM-IV features (delta_P, PI, etc.)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Added UNISIM-IV features (delta_P, PI, etc.)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Nenhum valor inválido detectado em DataFrame.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Added UNISIM-IV features (delta_P, PI, etc.)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Nenhum valor inválido detectado em DataFrame.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Normalized DataFrame to canonical feature set\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Nenhum valor inválido detectado em DataFrame.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Normalized DataFrame to canonical feature set\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Normalized DataFrame to canonical feature set\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Normalized DataFrame to canonical feature set\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Nenhum valor inválido detectado em DataFrame.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Nenhum valor inválido detectado em DataFrame.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Nenhum valor inválido detectado em DataFrame.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Beginning process_chunks: total=3, chunk=3\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ]   → Batch 1/1 (size=3)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Launching chunk of 3 models\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Beginning process_chunks: total=3, chunk=3\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ]   → Batch 1/1 (size=3)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Beginning process_chunks: total=3, chunk=3\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Launching chunk of 3 models\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ]   → Batch 1/1 (size=3)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Beginning process_chunks: total=3, chunk=3\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Launching chunk of 3 models\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ]   → Batch 1/1 (size=3)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Beginning process_chunks: total=3, chunk=3\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Launching chunk of 3 models\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Beginning process_chunks: total=3, chunk=3\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ]   → Batch 1/1 (size=3)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ]   → Batch 1/1 (size=3)\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Launching chunk of 3 models\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] → Launching chunk of 3 models\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Extractor: {'type': 'tcn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Extractor: {'type': 'rnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Extractor: {'type': 'cnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Extractor: {'type': 'cnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Fuser: {'type': 'bias_scale'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Fuser: {'type': 'film'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Fuser: {'type': 'film'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Fuser: {'type': 'bias_scale'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Extractor: {'type': 'tcn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Extractor: {'type': 'rnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Fuser: {'type': 'film'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] Fuser: {'type': 'bias_scale'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:42 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:43 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:43 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:43 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:37:43 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:04 [INFO   ] No suitable Dense layers found for analysis.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:04 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:04 [INFO   ] Extractor: {'type': 'cnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:04 [INFO   ] Fuser: {'type': 'bias_scale'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:04 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:29 [INFO   ] No suitable Dense layers found for analysis.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:29 [INFO   ] No suitable Dense layers found for analysis.\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:29 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:29 [INFO   ] Extractor: {'type': 'cnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:29 [INFO   ] Fuser: {'type': 'film'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:29 [INFO   ] training_mode: traditional\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:30 [INFO   ] Strategy: {'strategy_name': 'pressure_ensemble'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:30 [INFO   ] Extractor: {'type': 'cnn'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:30 [INFO   ] Fuser: {'type': 'bias_scale'}\u001b[0m\n",
      "\u001b[34m2025-06-06 08:38:30 [INFO   ] training_mode: traditional\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %%capture capturado\n",
    "def configure_logging():\n",
    "    \"\"\"Set up a colored logger respecting the global LOG_LEVEL.\"\"\"\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(\n",
    "        ColoredFormatter(\n",
    "            \"%(log_color)s%(asctime)s [%(levelname)-7s] %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            log_colors={\n",
    "                'DEBUG':    'cyan',\n",
    "                'INFO':     'blue',\n",
    "                'WARNING':  'yellow',\n",
    "                'ERROR':    'red',\n",
    "                'CRITICAL': 'bold_red',\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    root = logging.getLogger()\n",
    "    root.handlers = [handler]\n",
    "    # Map 0→WARNING, 1→INFO, 2→DEBUG\n",
    "    level = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}\n",
    "    root.setLevel(level.get(LOG_LEVEL, logging.INFO))\n",
    "\n",
    "\n",
    "\n",
    "# ─── executa tudo ───────────────────────────────────────────────\n",
    "def main(\n",
    "    ensemble_models: int = 1,\n",
    "    filter_methods: list = None,\n",
    "    selected_sources: list = None\n",
    ") -> dict:\n",
    "    \"\"\"End-to-end pipeline: select sources, iterate configs, run and collect all results.\"\"\"\n",
    "    logging.info(\"Starting main pipeline…\")\n",
    "    sources = select_data_sources(DATA_SOURCES, selected_sources)\n",
    "    if not sources:\n",
    "        logging.warning(\"No data sources selected; exiting.\")\n",
    "        return {}\n",
    "\n",
    "    results = {}\n",
    "    for cfg in create_filter_configurations(filter_methods):\n",
    "        results.update(run_experiments_for_config(cfg, sources, ensemble_models))\n",
    "\n",
    "    logging.info(\"Pipeline finished.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configure_logging()\n",
    "\n",
    "    # User parameters\n",
    "    ensemble_size = 3\n",
    "\n",
    "    # Run experiments\n",
    "    results = main(\n",
    "        ensemble_models=ensemble_size,\n",
    "        filter_methods=None,\n",
    "        selected_sources=DEFAULT_DATASET\n",
    "    )\n",
    "\n",
    "\n",
    "    # Save to Excel\n",
    "    exp_name = generate_experiment_name(\n",
    "        DEFAULT_DATASET,\n",
    "        DEFAULT_EXP_PARAMS[\"architecture_name\"],\n",
    "        ensemble_size\n",
    "    )\n",
    "    \n",
    "    output_path = save_experiment_to_excel(\n",
    "        DEFAULT_EXP_PARAMS,\n",
    "        results,\n",
    "        exp_name,\n",
    "        DEFAULT_DATASET,\n",
    "        ensemble_size,\n",
    "    )\n",
    "    logging.info(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085fe9b-df1e-44a8-9cec-30073eb68a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "myenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
